# Configuration file for the pipeline server.

# Relative paths are resolved with respect to this file location.

# Location of the server, communication occurs via TCP.
address = "127.0.0.1:12345"

# The directory where the client is expected to send the files to process.
# The `copy_to_server` command on the client side must copy the file to
# process to that directory with the name `{server_filename}`.
incoming_directory = "./server/buckets"

# Permission mode to use for "buckets" subdirectories in the
# `incoming_directory`. This only has an effect on unix platforms. Uncomment to
# set a value, otherwise system default is used.
# unix_mode = 0o755

# Processing command to apply to incoming files.
#
# This can be either:
# - an external command given as a list of strings representing the program
#   name and its arguments;
# - a `{ create_directory: "path" }` directive;
# - a `{ delete_file: "path" }` directive;
# - a `{ delete_directory: "path" }` directive;
# - a list where each element is either of the previous.
#
# The following placeholders are replaced at runtime:
# - `{server_path}` is the path of the file on the server;
# - `{client_name}` is the name of the client as defined in the client
#   configuration file;
# - `{client_relative_directory}` is the path to the file on the client,
#   relative to the watched directory;
# - `{client_file_stem}` is the file name on the client without its extension;
# - `{hash}` is a unique hash identifying the file. Using it as part of the
#   output filename of your processing command guarantees its uniqueness, so
#   that processing different files does not overwrite output.
processing = [
    { create_directory = "./server/{client_relative_directory}" },
    [ "cp", "{server_path}", "./server/{client_relative_directory}/{client_file_stem}.out" ],
]

# Whether a job is marked as done/failed based on `processing` exit status.
#
# When this option is set to false, the job is still internally marked as being
# processed after the `processing` command has been executed. The job has to be
# manually marked as done/failed by calling
# `pipeline server mark {hash} done|failed`
# Setting this option to `false` is intended for cases when the `processing`
# command above doesn't actually perform the desire processing but instead
# schedules it for execution (e.g. via a SLURM queue).
auto_status_update = true

# Period in seconds at which failed tasks should be retried.
retry_tasks_every_secs = 60

[concurrency]
# Maximum concurrent computations of file hashes.
max_hashes = 3
# Maximum spawns of the `processing` command.
max_processing = 8
